---
tags:
  - chapter
---

### Dodatkowe rzeczy, o których można wspomnieć:
- Podstawowe definicje np. patrz: https://arxiv.org/pdf/2311.05232 (Definitions)
- 

### Pomysły:

## [[LLM Evaluation|Ewaluacja LLMów]]
##### Co to jest? - coś bardziej ogólnego niż [[LLM Benchmarks]] xD
##### Dlaczego jest potrzebne?
- różnorodność zastosowań - znaleźć statystyki do jakich celów używane są LLMy
##### Jakie są metryki ewaluacyjne?
##### Jakie metryki są używane? Dlaczego takie metryki? ([[#LLM-judged metric]])
###### LLM-judged metric
Metryka, w której LLM (np. GPT-4o) przypisuje score danemu outputowi innego LLMa, na bazie `ground_truth` oraz `context` ([[LLM Evaluation#Judge|zobacz]])

Jak ewaluacja ma się do tego problemu, który badam?

## [[LLM Benchmarks|LLM Benchmarking]]
##### Jak ma się do [[LLM Evaluation|ewaluacji]]
##### Co to jest? - szczególny rodzaj ewaluacji - ustandaryzowany framework ewaluacyjny
##### Dlaczego jest potrzebne? - do porównywania llmów i fine tuningu
##### Co wchodzi w skład benchmarka (dataset, metryki ewaluacyjne i mechanizm punktowania)
#### Wyzwania w benchmarkingu LLMów
Opisać jakie są wyzwania i problemy przy walidowaniu LLMów
https://youtu.be/DeIUJRd48fI?t=1220
[benchmarking limitations IBM](https://www.ibm.com/think/topics/llm-benchmarks#:~:text=metrics%20is%20needed.-,Limitations%20of%20LLM%20benchmarks,-While%20benchmarks%20are)
Opisać takie problemy jak:
- overfitting (model radzi sobie dobrze na danych testowych ale ma problemy z realnymi przypadkami użycia)
##### Jak benchmarking ma się do problemu, który badam? - dosłownie będę tworzył własny benchmark

## [[Prompt engineering]] - znaczenie promptów przy ewaluacji
##### Co to jest?
##### Metody promptingu wykorzystywane w [[LLM Evaluation|ewaluacji LLMów]] i benchmarkingu
- Opisać jakie metody kiedy i dlaczego są stosowane? (np. że jeśli jest usecase AI asystenta to testujemy przy uzyciu few-shot promping/in-context learning, w innych przypadkach coś innego) - wtedy w [[4.Część praktyczna]] można uzasadnić dlaczego w naszym bench marku użwamy zero-shot promptingu (jeśli się na to zdecyduje)
- tutaj można opisać metody promptingu, które zostaną wykorzystane do [[4.Część praktyczna|eksperymentów]] (np. zero-shot prompting, in-context-learning, Chain-of-Thought - [[Prompt engineering]])
##### Znaczenie [[Prompt engineering#LLM Settings|ustawień LLMa]] w ewaluacji
Opisać wybrane ustawienia i dlaczego warto lub nie warto ich ruszać

## ??
#### Estymacja kosztów
Tutaj można opisać:
czym są tokeny, oraz jakieś obliczenia estymujące ile zapłacilibyśmy za przetestowanie całego datasetu (np. 1000 przykładów)
