### Pomysły:
#### Wprowadzenie do [[LLM Evaluation|ewaluacji]] i [[LLM Benchmarks|benchmarkingu]] LLMów
Co to jest?
Dlaczego jest potrzebne?
Jakie metryki są używane? Dlaczego takie metryki? ([[#LLM-judged metric]])
Jak ewaluacja ma się do tego problemu, który badam?
#### LLM-judged metric
Metryka, w której LLM (np. GPT-4o) przypisuje score danemu outputowi innego LLMa, na bazie `ground_truth` oraz `context` ([[LLM Evaluation#Judge|zobacz]])
#### [[Prompt engineering]] - znaczenie promptów przy ewaluacji
#### Metody promptingu wykorzystywane w [[LLM Evaluation|ewaluacji LLMów]]
Opisać jakie metody kiedy i dlaczego są stosowane?
tutaj można opisać metody promptingu, które zostaną wykorzystane do [[4.Część praktyczna|eksperymentów]] (np. zero-shot prompting, in-context-learning, Chain-of-Thought - [[Prompt engineering]])

#### Znaczenie [[Prompt engineering#LLM Settings|ustawień LLMa]] w ewaluacji
Opisać wybrane ustawienia i dlaczego warto lub nie warto ich ruszać
#### Wyzwania w ewaluowaniu LLMów
Opisać jakie są wyzwania i problemy przy walidowaniu LLMów
https://youtu.be/DeIUJRd48fI?t=1220
#### Estymacja kosztów
Tutaj można opisać:
czym są tokeny, oraz jakieś obliczenia estymujące ile zapłacilibyśmy za przetestowanie całego datasetu (np. 1000 przykładów)
