---
tags:
  - chapter
---

TBD
### Pomysły:

#### Dobór metryk i statystyk
Zdecydować jakie [[LLM Evaluation#Metrics|metryki]] będą stosowane i dlaczego np.:
- metryki
	- correctness - chemy sprawdzić czy LLM dobrze rozwiązał problem logiczny ([[3.Część teoretyczna#LLM-judged metric|LLM-judged metric]])
	- relevancy - może być przydatne gdy odpowieć llma jest totalnie z czapy, wtedy możemy zobaczyć, czy przynajmniej odpowiedział na temat
- statystyki
	- ratio - długość treści a poprawność odpowiedzi, żeby zobaczyć czy im dłuższa treść/dłuższa odpowiedź (ta co ma być poprawna) tym gorzej sobie radzą te modele
	- by difficulty - jak dobrze radziły sobie modele pod względem trudności problemów logicznych
	- Latency - jak długo trwała [inferencja](https://www.snowflake.com/guides/llm-inference/#:~:text=LLM%20inference%20is%20the%20mechanism,to%20generate%20human%2Dlike%20responses.)
	- Memory usage 
		- konsumpcja GPU
		- konsumpcja CPU
	- Koszt - względem [[3.Część teoretyczna#Estymacja kosztów|estymacji kosztów]]
#### Definiowanie customowej metryki
Trzeba będzie zbudować customową metrykę ([[3.Część teoretyczna#LLM-judged metric|LLM-judged metric]]) - przykład [patrz tutaj](https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html) i [tutaj](https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html).

#### Tworzenie customowego datasetu
TRZEBA ZROBIĆ WŁASNY [[Dataset]] - można powiedzieć, że jest bardziej "curated" niż [[LLM Benchmarks#Papers|istniejące benchmarki]]

#### Wybór [[LLM Models|modeli]] biorących udział w ewaluacji
Można opisać to, że sens ma branie pod uwagę tylko modeli, które mają dobry performance w podobnych [[LLM Benchmarks|benchmarkach]] + [tutaj gość o tym gada](https://youtu.be/DeIUJRd48fI?t=1314)
#### Evaluation Flow
Opisać flow ewaluacji z jakimś przyjemnym [[LLM Evaluation#Diagrams|diagramem]] (rezultaty będą walidowane przez LLM judge)

#### Dobór [[Prompt engineering|technik promptowania]]
Opisać jakie zostały wybrane i dlaczego?
#### Wybór technologii
Opisać że jest wiele technologii ([[Technologies]] + https://www.youtube.com/watch?v=DeIUJRd48fI&t=1911s) ale, że zdecydowałem się na takie i takie i podać powód.

Można wzorować się na np. [takiej implementacji](https://github.com/declare-lab/LLM-PuzzleTest)

Tak naprawdę część praktyczna sprowadzi się do stworzenia mojego własnego [[LLM Benchmarks|benchmarka dla LLMów]]

W jaki sposób [[3.Część teoretyczna#Wyzwania w ewaluowaniu LLMów|te problemy]] zostały zaadresowane?

TESTY MANUALNE: Można zrobić testy manualne i je opisać np. każąc modelowi opisywać swój "reasoning"

TESTY AUTOMATYCZNE: można napisać skrypt, który każe różnym modelom rozwiązywać wiele różnych zadań. Potem policzy ile dany model rozwiązał dobrze i można wyniki przedstawić w formie fajnego i przyjemnego wykresu słupkowego (POPRAWNOŚĆ ODPOWIEDZI BĘDZIE TRZEBA WALIDOWAĆ Z WYKORZYSTANIEM LLM'a? - [[LLM Evaluation]])

[[LLM Evaluation|Ewaluacja]] to powinien być zero-shot prompting raczej, żeby łatwo zautomatyzować testy

Można by było "nafeedować" jeden model wiedzą na temat propositional logic i zobaczyć czy radzi sobie lepiej? Albo sprawdzić na jakimś modelu stricte matematycznym?? - [[Fine Tuning of LLMs]]

Może są jakieś gotowe [[Technologies|technologie]], które można wykorzystać do przeprowadzenia eksperymentów?

Ogólne flow eksperymentów można opisać jako:
![[Pasted image 20241021134134.png]]

Można by pokazać, że 